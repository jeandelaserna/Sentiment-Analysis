{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recuperation_tweet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUIge-icpetz",
        "colab_type": "text"
      },
      "source": [
        "# Recuperation et analyse de donnée Twitter\n",
        "\n",
        "Ici nous allons utiliser la bibliotèque tweepy pour effectuer des recherche et recuperer un flux twitter et ensuite nous verrons la methode pour installer tout ceci sur un raspberrypi pi pour automatiser la recuperation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmcpfRCTdiGU",
        "colab_type": "text"
      },
      "source": [
        "En premier lieu il nous faut des clé d'identification.\n",
        "Ces clés sont obtenables en s'incrivant sur twitter et créant une app twitter ( https://developer.twitter.com/en/apps ) et sont necaissaire pour l'utilisation de la bibliothèque."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eu2Du39-de36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "consumer_key= 'ErI2gyTmnaE3YGeAz4DWc62nX'\n",
        "consumer_secret= 'OzSy0s4ma6CDVxwZgLisFdqmguDLQrsYaxqpCEqE31He8UgupR'\n",
        "access_token= '1229729717806358529-iXdWrMVhtYT8NuPQyT8ldOTr1k3QRz'\n",
        "access_token_secret= 'hDHXzyuAJFgvWAodA74KsJYtB9cUPKTP5CQ7snCWDX3Hh'"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbtxOxgMjAI5",
        "colab_type": "text"
      },
      "source": [
        "## Imports et Fonctions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsuOrWlFjPKr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "65cc87e3-58b2-4006-f826-4f5c1afddc47"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import re, string, random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "import pickle\n",
        "\n",
        "import tweepy as tw\n",
        "from tweepy import Stream\n",
        "from tweepy import OAuthHandler\n",
        "from tweepy.streaming import StreamListener\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, BatchNormalization\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT6K9lqgmNa7",
        "colab_type": "text"
      },
      "source": [
        "### Mise en place du Classifieur\n",
        "\n",
        "Les classifieurs choisi sont 4 clasiifieurs entrainés sur le dataset reduit avec le reseau de neuronne pré-entrainé du notebook reduction.Et ensuite on prend la classe la plus predite par ces classifieur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxwckGvp-2z5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_noise(tweet_tokens, stop_words = ()): \n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens\n",
        "\n",
        "\n",
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "def make_set( tweet_tokens, label=\"\", stop_words = () ) :\n",
        "    cleaned_tokens_list = []\n",
        "    for tokens in tweet_tokens:\n",
        "        cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "    tokens_for_model = get_tweets_for_model(cleaned_tokens_list)\n",
        "    dataset = [(tweet_dict, label)for tweet_dict in tokens_for_model]\n",
        "    return dataset\n",
        "\n",
        "def to_numpy(dataset,vocab=None):\n",
        "\n",
        "    V = [] #vacobulaire\n",
        "    Data = [] #features\n",
        "    C = [] #classe\n",
        "\n",
        "    if vocab==None:\n",
        "\n",
        "#vacobulaire     \n",
        "      for tweet in dataset:\n",
        "        for token in tweet[0].keys():\n",
        "          V.append(token)\n",
        "      V = np.unique(V)\n",
        "\n",
        "      for tweet in dataset:\n",
        "#features\n",
        "        x = np.zeros(V.shape)\n",
        "        for token in tweet[0].keys():\n",
        "          x += (token == V)\n",
        "        Data.append(x)\n",
        "#classe\n",
        "        if tweet[1] == 'Positive':\n",
        "          C.append([1,0,0])\n",
        "        elif tweet[1] == 'Negative':\n",
        "          C.append([0,1,0])\n",
        "        else :\n",
        "          C.append([0,0,1])\n",
        "\n",
        "    else:\n",
        "      V = np.array(vocab)\n",
        "      for tweet in dataset:\n",
        "  #features\n",
        "        x = np.zeros(V.shape)\n",
        "        for token in tweet[0].keys():\n",
        "          x += (token == V)\n",
        "        Data.append(x)\n",
        "  #classe\n",
        "        if tweet[1] == 'Positive':\n",
        "          C.append([1,0,0])\n",
        "        elif tweet[1] == 'Negative':\n",
        "          C.append([0,1,0])\n",
        "        else :\n",
        "          C.append([0,0,1])\n",
        "\n",
        "    \n",
        "    Xtrain = Data[:11000]\n",
        "    Xtest = Data[11000:]\n",
        "    Xtrain = (np.array(Xtrain)!=0) *1\n",
        "    Xtest = (np.array(Xtest)!=0) *1\n",
        "\n",
        "    Ytrain = np.array(C[:11000])\n",
        "    Ytest = np.array(C[11000:])\n",
        "\n",
        "    return Xtest,Xtrain,Ytest,Ytrain,V\n",
        "\n",
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "neutral_tweet_tokens = np.random.choice(twitter_samples.tokenized('tweets.20150430-223406.json'),size=len(negative_tweet_tokens),replace = False)\n",
        "\n",
        "positive_dataset = make_set( positive_tweet_tokens, label=\"Positive\", stop_words = stop_words )\n",
        "negative_dataset = make_set( negative_tweet_tokens, label=\"Negative\", stop_words = stop_words )\n",
        "neutral_dataset = make_set( neutral_tweet_tokens, label=\"Neutral\", stop_words = stop_words )\n",
        "\n",
        "dataset = positive_dataset + negative_dataset + neutral_dataset\n",
        "random.shuffle(dataset)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sQFxxNKu5Cw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53e0d68f-5cd7-4812-c14c-4f4363b6beda"
      },
      "source": [
        "#reduction de la dimention\n",
        "with open(\"vocab_reduc_part0.txt\", \"rb\") as f:\n",
        "    V = pickle.load(f)\n",
        "\n",
        "Xtest,Xtrain,Ytest,Ytrain,V = to_numpy(dataset,vocab=V)\n",
        "part0 = load_model('reduc_part0.h5')\n",
        "\n",
        "Xtest_red,Xtrain_red = np.array(part0(Xtest)),np.array(part0(Xtrain))\n",
        "Ytest,Ytrain = (Ytest[:,0]==1)*1 + (Ytest[:,1]==1)*2 + (Ytest[:,2]==1)*3, (Ytrain[:,0]==1)*1+(Ytrain[:,1]==1)*2+(Ytrain[:,2]==1)*3"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8R-xStYVpHei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#entrainement des classifieurs\n",
        "CL = [GaussianNB(),\n",
        "      DecisionTreeClassifier(max_depth=4),\n",
        "      SVC(),\n",
        "      MLPClassifier(hidden_layer_sizes=(10,10,10,10,10),max_iter=5000), ]\n",
        "\n",
        "NCL = ['GaussianNB',\n",
        "       'DecisionTreeClassifier profondeur 4',\n",
        "       'SVC kernel rbf',\n",
        "       'MLPClassifier 5 couches cachées',]\n",
        "\n",
        "for clf in CL:\n",
        "    clf_sentiment = clf.fit(Xtrain_red,Ytrain)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6JPQ1Peou51",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "d44bfdf5-9bae-4605-a823-8c3be4090466"
      },
      "source": [
        "#mise en place de la methode ensembliste\n",
        "eclf = VotingClassifier(estimators=[ (NCL[i],CL[i]) for i in range(len(NCL)) ],\n",
        "                        voting='hard')\n",
        "\n",
        "eclf = eclf.fit(Xtrain_red,Ytrain)\n",
        "\n",
        "y_pred = eclf.predict(Xtest_red)\n",
        "\n",
        "print(metrics.classification_report(Ytest,y_pred))\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.99      1.00      0.99      1334\n",
            "           2       1.00      0.99      0.99      1295\n",
            "           3       0.99      0.99      0.99      1371\n",
            "\n",
            "    accuracy                           0.99      4000\n",
            "   macro avg       0.99      0.99      0.99      4000\n",
            "weighted avg       0.99      0.99      0.99      4000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qZybnbEGp-W",
        "colab_type": "text"
      },
      "source": [
        "## Recherche twitter\n",
        "\n",
        "Nous allons voir quelques exemple de recherche de tweet sur quelques sujets. Pour plus de detail sur les filtre de recherche utilisable https://developer.twitter.com/en/docs/tweets/rules-and-filtering/overview/standard-operators.\n",
        "\n",
        "Il est bon de savoir aussi qu'il y à plusieurs contraintes, sur cet outil de recherche:\n",
        " \n",
        "* La limite en nombre de requete est 180/15 minutes et on ne peu aller au-delà de 7 jour pour un sujet donné.\n",
        "* Pour recuperer la timeline d'un user la limite est de 900/15 minutes,avec 200/par requette sans la limite de es 7 jour. Mais l'api tweeter ne semble pas avoir d'outils pour aller plus vieux que les 200 plus recents.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbXjjitKPFM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tw.API(auth, wait_on_rate_limit=False)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnwkWKOXN49w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 20\n",
        "search_words = \"New York  -filter:retweets\"\n",
        "date_since = \"2020-01-01\"\n",
        "R = tw.Cursor(api.search,q=search_words,lang=\"en\",since=date_since,result_type = \"recent\" ).items(N)\n",
        "\n",
        "infos = []\n",
        "\n",
        "V = np.array(V)\n",
        "for tweet in R :\n",
        "\n",
        "  Txt = remove_noise(word_tokenize(tweet.text))\n",
        "  txt_vec = np.zeros(np.shape(V))\n",
        "  for token in Txt:\n",
        "    txt_vec += token == V\n",
        "  txt_vec = txt_vec!=0\n",
        "\n",
        "  txt_vec_red = np.array(part0(txt_vec.reshape(1,-1)))\n",
        "\n",
        "  s_tweet = eclf.predict(txt_vec_red) \n",
        "  if s_tweet==1 :\n",
        "    s_tweet = \"positif\"\n",
        "  elif s_tweet==2 :\n",
        "    s_tweet = \"negatif\"\n",
        "  else:\n",
        "    s_tweet = \"neutre\"\n",
        "    \n",
        "  infos.append([tweet.created_at ,\n",
        "              tweet.author.screen_name,\n",
        "              tweet.author.location,\n",
        "              tweet.author.followers_count,\n",
        "              tweet.text,\n",
        "              s_tweet,\n",
        "              ])\n",
        "\n",
        "Mesure = pd.DataFrame(data=infos,columns=[\"time\",\"screen_name\", \"location\",\"followers_count\",\"text\",\"sentiment_tweet\"],)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkmlAbm2qlBq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "9234b217-7065-4004-ca6e-4e30464c98c8"
      },
      "source": [
        "Mesure"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>screen_name</th>\n",
              "      <th>location</th>\n",
              "      <th>followers_count</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-08-24 08:58:01</td>\n",
              "      <td>Juice_Berry80</td>\n",
              "      <td>Perth Australia</td>\n",
              "      <td>89</td>\n",
              "      <td>@gfollis @tonilontis @MagdaSzubanski Controls ...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-08-24 08:58:00</td>\n",
              "      <td>livetennis</td>\n",
              "      <td>Tennis!</td>\n",
              "      <td>519948</td>\n",
              "      <td>Novak Djokovic played his last ATP match on 29...</td>\n",
              "      <td>negatif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-08-24 08:57:53</td>\n",
              "      <td>devdelevingne</td>\n",
              "      <td>Long Island, NY</td>\n",
              "      <td>4378</td>\n",
              "      <td>i’m literally just a dummy for new york 🤡</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-08-24 08:57:52</td>\n",
              "      <td>newmystic</td>\n",
              "      <td>Fort Worth, TX</td>\n",
              "      <td>64</td>\n",
              "      <td>@ANNELAMOTT oh, those real estate ones like Mi...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-08-24 08:57:44</td>\n",
              "      <td>511NY</td>\n",
              "      <td>New York State</td>\n",
              "      <td>2592</td>\n",
              "      <td>Cleared: Incident on #CoastLine at New York Pe...</td>\n",
              "      <td>positif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020-08-24 08:57:43</td>\n",
              "      <td>511NY</td>\n",
              "      <td>New York State</td>\n",
              "      <td>2592</td>\n",
              "      <td>Cleared: Construction on #NortheastLine Both d...</td>\n",
              "      <td>negatif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2020-08-24 08:57:43</td>\n",
              "      <td>511NYC</td>\n",
              "      <td>New York City</td>\n",
              "      <td>2484</td>\n",
              "      <td>Cleared: Construction on #NortheastLine Both d...</td>\n",
              "      <td>negatif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2020-08-24 08:57:43</td>\n",
              "      <td>511NY</td>\n",
              "      <td>New York State</td>\n",
              "      <td>2592</td>\n",
              "      <td>Cleared: Construction on #MidtownDirect Both d...</td>\n",
              "      <td>negatif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2020-08-24 08:57:43</td>\n",
              "      <td>511NYC</td>\n",
              "      <td>New York City</td>\n",
              "      <td>2484</td>\n",
              "      <td>Cleared: Construction on #MidtownDirect Both d...</td>\n",
              "      <td>negatif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2020-08-24 08:57:43</td>\n",
              "      <td>511NY</td>\n",
              "      <td>New York State</td>\n",
              "      <td>2592</td>\n",
              "      <td>Cleared: Construction on #NorthLine Both direc...</td>\n",
              "      <td>negatif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2020-08-24 08:57:43</td>\n",
              "      <td>511NYC</td>\n",
              "      <td>New York City</td>\n",
              "      <td>2484</td>\n",
              "      <td>Cleared: Construction on #NorthLine Both direc...</td>\n",
              "      <td>negatif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2020-08-24 08:57:40</td>\n",
              "      <td>quinn0brown</td>\n",
              "      <td>near hf</td>\n",
              "      <td>765</td>\n",
              "      <td>night out in new york 🚧 https://t.co/PAffvvhate</td>\n",
              "      <td>positif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2020-08-24 08:57:39</td>\n",
              "      <td>palindrome_lndn</td>\n",
              "      <td>London, United Kingdom</td>\n",
              "      <td>302</td>\n",
              "      <td>@Francesbarnby @johnleremainer @DSJ51 And John...</td>\n",
              "      <td>positif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2020-08-24 08:57:32</td>\n",
              "      <td>enerphyschem</td>\n",
              "      <td>CitYZen AutoDidAct</td>\n",
              "      <td>290</td>\n",
              "      <td>Absurd doubt: Comparing population (gen[etics]...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2020-08-24 08:57:25</td>\n",
              "      <td>LuisSuarezNY</td>\n",
              "      <td>Manhattan, NY</td>\n",
              "      <td>2235</td>\n",
              "      <td>The most beautiful storage facility in New Yor...</td>\n",
              "      <td>positif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2020-08-24 08:57:19</td>\n",
              "      <td>hassciencebot</td>\n",
              "      <td></td>\n",
              "      <td>18</td>\n",
              "      <td>Has The New York Times given stupid boys too far?</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2020-08-24 08:57:14</td>\n",
              "      <td>Fatfartcicle</td>\n",
              "      <td>Guadalupe, AZ</td>\n",
              "      <td>1</td>\n",
              "      <td>2:22 movie : This is a chick flick masqueradin...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2020-08-24 08:57:13</td>\n",
              "      <td>sixstreetunder</td>\n",
              "      <td></td>\n",
              "      <td>2773</td>\n",
              "      <td>New York - October 2018 https://t.co/qZfaX0R7X...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2020-08-24 08:57:06</td>\n",
              "      <td>twicexxonce__</td>\n",
              "      <td>트와이스•방탄사냔단</td>\n",
              "      <td>154</td>\n",
              "      <td>@billboard_twice favorite city: New York city\\...</td>\n",
              "      <td>negatif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2020-08-24 08:57:04</td>\n",
              "      <td>nhillery1</td>\n",
              "      <td>Durham, NC</td>\n",
              "      <td>3718</td>\n",
              "      <td>“Trump is running his 2020 campaign like an an...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  time  ... sentiment_tweet\n",
              "0  2020-08-24 08:58:01  ...          neutre\n",
              "1  2020-08-24 08:58:00  ...         negatif\n",
              "2  2020-08-24 08:57:53  ...          neutre\n",
              "3  2020-08-24 08:57:52  ...          neutre\n",
              "4  2020-08-24 08:57:44  ...         positif\n",
              "5  2020-08-24 08:57:43  ...         negatif\n",
              "6  2020-08-24 08:57:43  ...         negatif\n",
              "7  2020-08-24 08:57:43  ...         negatif\n",
              "8  2020-08-24 08:57:43  ...         negatif\n",
              "9  2020-08-24 08:57:43  ...         negatif\n",
              "10 2020-08-24 08:57:43  ...         negatif\n",
              "11 2020-08-24 08:57:40  ...         positif\n",
              "12 2020-08-24 08:57:39  ...         positif\n",
              "13 2020-08-24 08:57:32  ...          neutre\n",
              "14 2020-08-24 08:57:25  ...         positif\n",
              "15 2020-08-24 08:57:19  ...          neutre\n",
              "16 2020-08-24 08:57:14  ...          neutre\n",
              "17 2020-08-24 08:57:13  ...          neutre\n",
              "18 2020-08-24 08:57:06  ...         negatif\n",
              "19 2020-08-24 08:57:04  ...          neutre\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KRCF_IDRNEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N = 20\n",
        "search_words = \"from:realDonaldTrump -filter:retweets\"\n",
        "date_since = \"2020-01-01\"\n",
        "\n",
        "R = tw.Cursor(api.search,q=search_words,lang=\"en\",since=date_since,result_type = \"recent\" ).items(N)\n",
        "\n",
        "infos = []\n",
        "\n",
        "V = np.array(V)\n",
        "for tweet in R :\n",
        "\n",
        "  Txt = remove_noise(word_tokenize(tweet.text))\n",
        "  txt_vec = np.zeros(np.shape(V))\n",
        "  for token in Txt:\n",
        "    txt_vec += token == V\n",
        "  txt_vec = txt_vec!=0\n",
        "\n",
        "  txt_vec_red = np.array(part0(txt_vec.reshape(1,-1)))\n",
        "\n",
        "  s_tweet = eclf.predict(txt_vec_red) \n",
        "  if s_tweet==1 :\n",
        "    s_tweet = \"positif\"\n",
        "  elif s_tweet==2 :\n",
        "    s_tweet = \"negatif\"\n",
        "  else:\n",
        "    s_tweet = \"neutre\"\n",
        "    \n",
        "  infos.append([tweet.created_at ,\n",
        "              tweet.author.screen_name,\n",
        "              tweet.author.location,\n",
        "              tweet.author.followers_count,\n",
        "              tweet.text,\n",
        "              s_tweet,\n",
        "              ])\n",
        "\n",
        "Mesure = pd.DataFrame(data=infos,columns=[\"time\",\"screen_name\", \"location\",\"followers_count\",\"text\",\"sentiment_tweet\"],)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKuTNOIGtthG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "outputId": "649f9b6f-32b6-4244-f25c-4e9c2192fcac"
      },
      "source": [
        "Mesure"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>screen_name</th>\n",
              "      <th>location</th>\n",
              "      <th>followers_count</th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-08-24 02:00:38</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Continued... https://t.co/HNNyF3B5NM</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-08-24 01:59:24</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Please watch highly respected Dr. Harvey Risch...</td>\n",
              "      <td>negatif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-08-24 01:23:25</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>The greatest Election Fraud in our history is ...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-08-24 00:02:24</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Watch Mark Levin, on air now. Great! @FoxNews</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-08-23 18:45:19</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Actually, I think I’m leading in the Polls!  @...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020-08-23 18:37:58</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Chris Wallace “forgot” to ask a very weak and ...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2020-08-23 18:29:03</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>“Next we’ll turn to people of the Biden inner ...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2020-08-23 17:33:53</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Important White House News Conference at 5:30 ...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2020-08-23 13:48:46</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Happy Sunday! We want GOD! \\nhttps://t.co/RsBk...</td>\n",
              "      <td>positif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2020-08-23 13:11:59</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Two Democrat Caucus Meetings removed “UNDER GO...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2020-08-23 12:13:52</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>These riots are an antigovernment movement fro...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2020-08-23 11:25:59</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>So now the Democrats are using Mail Drop Boxes...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2020-08-23 01:26:47</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Now they’re trying to deny this, but they can’...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2020-08-23 00:35:35</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>.@LouDobbs wrote a great new book, “The Trump ...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2020-08-23 00:28:28</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>A “Favored Nations Clause” against Big Pharma,...</td>\n",
              "      <td>positif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2020-08-22 22:52:05</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Will be interviewed by @SteveHiltonx on @FoxNe...</td>\n",
              "      <td>positif</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2020-08-22 22:17:09</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Despite biggest ever job gains and a V shaped ...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>2020-08-22 22:07:33</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>Terrible! https://t.co/pzCR7ctciz</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2020-08-22 22:05:33</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>We did Criminal Justice Reform without many po...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>2020-08-22 20:51:34</td>\n",
              "      <td>realDonaldTrump</td>\n",
              "      <td>Washington, DC</td>\n",
              "      <td>85555361</td>\n",
              "      <td>....that they are trying to pull off in violat...</td>\n",
              "      <td>neutre</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  time  ... sentiment_tweet\n",
              "0  2020-08-24 02:00:38  ...          neutre\n",
              "1  2020-08-24 01:59:24  ...         negatif\n",
              "2  2020-08-24 01:23:25  ...          neutre\n",
              "3  2020-08-24 00:02:24  ...          neutre\n",
              "4  2020-08-23 18:45:19  ...          neutre\n",
              "5  2020-08-23 18:37:58  ...          neutre\n",
              "6  2020-08-23 18:29:03  ...          neutre\n",
              "7  2020-08-23 17:33:53  ...          neutre\n",
              "8  2020-08-23 13:48:46  ...         positif\n",
              "9  2020-08-23 13:11:59  ...          neutre\n",
              "10 2020-08-23 12:13:52  ...          neutre\n",
              "11 2020-08-23 11:25:59  ...          neutre\n",
              "12 2020-08-23 01:26:47  ...          neutre\n",
              "13 2020-08-23 00:35:35  ...          neutre\n",
              "14 2020-08-23 00:28:28  ...         positif\n",
              "15 2020-08-22 22:52:05  ...         positif\n",
              "16 2020-08-22 22:17:09  ...          neutre\n",
              "17 2020-08-22 22:07:33  ...          neutre\n",
              "18 2020-08-22 22:05:33  ...          neutre\n",
              "19 2020-08-22 20:51:34  ...          neutre\n",
              "\n",
              "[20 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79njaNuQpENv",
        "colab_type": "text"
      },
      "source": [
        "## streaming de twitter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02-gf47tmyyw",
        "colab_type": "text"
      },
      "source": [
        "Pour recuperer le flux twitter et l'nregistrer il va nous falloir plusieurs fonctions et quelques imports:\n",
        "\n",
        "    from tweepy.streaming import StreamListener\n",
        "    import time\n",
        "    import json\n",
        "    import csv\n",
        "    import numpy as np\n",
        "    ckey='ErI2gyTmnaE3YGeAz4DWc62nX'\n",
        "    csecret='OzSy0s4ma6CDVxwZgLisFdqmguDLQrsYaxqpCEqE31He8UgupR'\n",
        "    atoken='1229729717806358529-iXdWrMVhtYT8NuPQyT8ldOTr1k3QRz'\n",
        "    asecret='hDHXzyuAJFgvWAodA74KsJYtB9cUPKTP5CQ7snCWDX3Hh'\n",
        "\n",
        "Les tweets et leurs information associée arrivent en format Json mais si les colonnes sont vide rien ne l'indique et il faut alors parcourir l'ensemble des information pour recuperer l'ensemble des colonnes:\n",
        "\n",
        "      def recup_keys(D):\n",
        "          keys = list(D[0].keys())\n",
        "          for i in range(1,len(D)):\n",
        "              k=list(D[i].keys())\n",
        "              for j in range(len(k)):\n",
        "                  keys.append(k[j])\n",
        "\n",
        "          keys = list(np.unique(keys))\n",
        "          return keys\n",
        "\n",
        "      def to_csv(D,name):\n",
        "          keys = recup_keys(D)\n",
        "          with open(name, 'a',encoding=\"utf-8\") as f:  \n",
        "              w = csv.DictWriter(f,fieldnames=keys)\n",
        "              w.writeheader()\n",
        "\n",
        "              for i in range(len(D)):\n",
        "                  w.writerow(D[i])\n",
        "\n",
        "Enfin voici le listener une sous-classe du StreamListener de tweepy ,avec 4 attributs suplementaires:\n",
        "* D un liste de chaques tweets et informations associées \n",
        "* name qui servira lors de l'enregistrement.\n",
        "* path le chemin d'accès pour l'enregistrement du csv, ne pas faire en chemin relatif cela évite de perdre du temps.\n",
        "* size taille de la liste, ici 10 tweets et ses informations mais modifiable pour l'adapter à sa quantité de RAM ou à la frequence des coupure internet ou de courant.\n",
        "\n",
        "La methode on_data filtre les retweet et lorsque la liste est remplie l'enregistre en format csv.\n",
        "                  \n",
        "      class listener(StreamListener):\n",
        "          \n",
        "          def __init__(self,name,path,size=10):\n",
        "              self.D = []\n",
        "              self.name = name\n",
        "              self.path = path\n",
        "              self.size = size\n",
        "\n",
        "          def on_data(self, data):\n",
        "              \n",
        "              all_data = json.loads(data)\n",
        "              tweet = all_data[\"text\"]\n",
        "\n",
        "              if tweet[0:2] != 'RT':\n",
        "                  self.D.append(all_data)\n",
        "                  #print('test')\n",
        "                  if len(self.D) == self.size:\n",
        "                      to_csv(self.D, self.path +'datatweet_' + str(self.name )+ str(time.time()) + '.csv')\n",
        "                      self.D=[]\n",
        "\n",
        "              return True\n",
        "\n",
        "          def on_error(self, status):\n",
        "              print (status)\n",
        "\n",
        "Et enfin pour recuper les donnée , dans un fichier tweetstreaming.py mettre les fonction precedente et les imports et faire un second fichier,par exemple recherche.py, dans le même repertoire:\n",
        "\n",
        "    import tweetstreaming as ts\n",
        "\n",
        "    from tweepy import Stream\n",
        "    from tweepy import OAuthHandler\n",
        "\n",
        "    auth = OAuthHandler(ts.ckey, ts.csecret)\n",
        "    auth.set_access_token(ts.atoken, ts.asecret)\n",
        "\n",
        "    twitterStream = Stream(auth, ts.listener(name=\"nom\"))\n",
        "    twitterStream.filter(track=[\"vos\",\"mots\",\"clé\"])\n",
        "\n",
        "Pour finir vous pouvez le lancer en ligne de commande dans windows ou linux ou dans un environnement de développement.\n",
        "Il aussi possible de le mettre en tache de fond, c'est ce que j'ai fait sur un raspberrypi. \n",
        "\n",
        "* Avoir un raspberrypi et installer la distibution linux raspbian(https://www.raspberrypi.org/documentation/installation/installing-images/) ou bien un ordinateur avec un os linux.\n",
        "* Mettre dans un même fichier tweetstreaming.py et recherche.py.\n",
        "* (facultatif)  Lancer un test avec un size petit et un mot clé très courant pour verifier si l'ensemble des fonction marche bien et la présence de toute les bibliothèque requise (le cas écheant les installer avec pip).\n",
        "* Editons ensuite /etc/rc.local et avant la ligne \n",
        "\n",
        "      exit 0\n",
        "\n",
        "  mettre:\n",
        "      /usr/bin/python3 /home/pi/monprojet/recherche.py &\n",
        "\n",
        "* Redémarrer, et le programme devrait etre lancé(Attention, un programme lancé de cette manière est un programme lancé en root, avec toutes les permissions en lecture comme en écriture).\n",
        "\n",
        "Le nombre de tweets est dépendant des mots clés, il peut être normal d'avoir des resultat qu'au bout d'un certain temps.\n",
        "\n",
        "Après ceci nous avons plusieurs fichiers du type datatweet_name_date.csv, alors pour les fusionner :\n",
        "\n",
        "      import pandas as pd\n",
        "      import os\n",
        "\n",
        "      rep = r\"path\"\n",
        "\n",
        "      Listecsv = os.listdir(rep)\n",
        "\n",
        "      a = pd.read_csv(rep + Listecsv[0])\n",
        "\n",
        "      for f in Listecsv[1:]:\n",
        "          \n",
        "          b = pd.read_csv(rep + f)\n",
        "          a = pd.concat((a,b))\n",
        "          \n",
        "      print(sum(a['lang']=='en'))\n",
        "\n",
        "      a.to_csv('sujetdataset.csv')\n",
        "\n",
        "      #a = pd.read_csv('sujetdataset.csv')\n",
        "\n",
        "il faudrat veiller à mettre l'ensemble des csv dans le même repertoire sans aucun autre fichier et en renseignant ce repertoire dans path, puis executer le code ci-dessus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgHB9jf_d0o-",
        "colab_type": "text"
      },
      "source": [
        "## Documentation Utilisé\n",
        "\n",
        "-https://www.earthdatascience.org/courses/use-data-open-source-python/intro-to-apis/twitter-data-in-python/\n",
        "\n",
        "-https://pythonprogramming.net/twitter-api-streaming-tweets-python-tutorial/\n",
        "\n",
        "-https://developer.twitter.com/en/docs/tweets/rules-and-filtering/overview/standard-operators\n",
        "\n",
        "-https://pythonprogramming.net/twitter-api-streaming-tweets-python-tutorial/\n",
        "\n",
        "-https://developer.twitter.com/en/docs/twitter-api/v1/tweets/post-and-engage/overview\n",
        "\n",
        "-documentation tweepy : http://docs.tweepy.org/en/latest/\n"
      ]
    }
  ]
}