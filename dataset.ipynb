{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skAHheaHo5Xn",
        "colab_type": "text"
      },
      "source": [
        "# FORMATION DU DATASET\n",
        "\n",
        "Ici nous nous allons travailler a construire un dataset grace à une suite de fonctions pour pretraiter des tweets. Le cadre d'utilisation de ces donnée sera l'analyse de sentiment mais on peut tout aussi bien adapter ce qui est developé ici à d'autres contextes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP4okbv2Awy3",
        "colab_type": "text"
      },
      "source": [
        "## Les Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWvQxTb0KJwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "595c212f-d81c-44c8-c818-29c45c5cc794"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import re, string, random\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "# les stop word sont des mots telque ‘ourselves’, ‘hers’, ‘between’, ‘yourself’, ‘but’, ... \n",
        "# des mots assez courant pour apparaitre dans n'import quel texte"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWoebmZRBtLo",
        "colab_type": "text"
      },
      "source": [
        "Tout d'abord importons le dataset, nltk met a disposition plusieurs sets de tweets en langue anglaise avec des labels de tweet neutres, positifs et negatifs, nous utliserons ce qui est developpé dans ce notebook pour l'apprentisage de nos classifieurs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzKp-7acLjy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import du dataset et tokenization\n",
        "\n",
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "neutral_tweet_tokens = np.random.choice(twitter_samples.tokenized('tweets.20150430-223406.json'),size=len(negative_tweet_tokens),replace = False)\n",
        "\n",
        "#pour l'utilisation d'un dataset non equilibré:\n",
        "#neutral_tweet_tokens = twitter_samples.tokenized('tweets.20150430-223406.json')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWciRnCJA_1t",
        "colab_type": "text"
      },
      "source": [
        "## Le Pré-traitement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEjVp_1iC1Qc",
        "colab_type": "text"
      },
      "source": [
        "Ci-dessous quelques fonctions qui permetrons de pré-traiter les tweets.\n",
        "\n",
        "* Nous utilison des tweet tokenisé, c'est dire que l'on passe d'une string a une liste de token, par exemple :\n",
        "  * \"les vache broute dans le champ\" devient [\"les\", \"vache\", \"broutent\", \"dans\",  \"le\", \"champ\"]\n",
        "\n",
        "* les tweets sont en premier lieu nettoyé des partie que nous jugeons dispensable les adresses de sites, les noms, les chiffres, les nombres, les ponctuations,... \n",
        "\n",
        "* Certain mots appelés stop words, sont aussi suprimés.ce sont des mot qui apparaise dans n'importe qule contexte, en francais ce serait les mot le, la, les, à, ... en englais ce sont leurs traductions ainsi que les auxiliaires utilisé pour les temps et d'autres plus specifiques à l'anglais.\n",
        "(pour le detail de ces mots, regarder le contenu de la liste stop_words)\n",
        "\n",
        "* et enfin les token restant sont lemmetizés c'est a dire dans le cas du francais la liste [\"cours\", \"courrant\", \"courrir\"] devient [\"cour\", \"cour\",\"cour\"] ou bien en anglais [\"run\", \"ran\", \"running\"] devient [\"run\", \"run\", \"run\"]\n",
        "\n",
        "Ainsi nous reduisons le dictionaire des mots utilisés sans pour autant perdre beaucoup de sens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rofD_vlKfS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_noise(tweet_tokens, stop_words = ()): #fait le nettoyage comme expliqué plus haut\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens\n",
        "\n",
        "\n",
        "def get_tweets_for_model(cleaned_tokens_list):# Nécessaire pour avoir le format requis pour les classifieurs de NLTK\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "def make_set( tweet_tokens, label=\"\", stop_words = () ) :# nettoie les tweets tokenisés et les associe à un label\n",
        "    cleaned_tokens_list = []\n",
        "    for tokens in tweet_tokens:\n",
        "        cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "    tokens_for_model = get_tweets_for_model(cleaned_tokens_list)\n",
        "    dataset = [(tweet_dict, label)for tweet_dict in tokens_for_model]\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls60ZGK2IFam",
        "colab_type": "text"
      },
      "source": [
        "Les defaults de cette manière de faire, sont :\n",
        "* ne prend pas en compte la position des mots dans la phrase.\n",
        "* les temps des phrases et les sens associés au terminaisons des mots sont effacé.\n",
        "* les sens d'expression telque \"Tomber dans les pommes\" peut disparaitre.\n",
        "* ...\n",
        "\n",
        "\n",
        "Cette manière de binariser un texte est une parmis d'autre , on pourrait aussi travailler avec des n-uple de mots, par exemple avec des couple de mots:\n",
        "* \"les vache broutent l'herbe \" devient [\"les vache\", \"les broutent\", \"les l'herbe\" ,\"vache broute\", \"vache l'herbe\", \"broute l'herbe\"]. \n",
        "\n",
        "Ces manières là peuvent reduire quelques defaults plus haut mais en apporte d'autre avec eux, une dimension plus grande des données, des combinaison de mot qui n'apparaissent pratiquement jamais,...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tW68Y2MKh85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  nettoyage et formation du dataset\n",
        "\n",
        "positive_dataset = make_set( positive_tweet_tokens, label=\"Positive\", stop_words = stop_words )\n",
        "negative_dataset = make_set( negative_tweet_tokens, label=\"Negative\", stop_words = stop_words )\n",
        "neutral_dataset = make_set( neutral_tweet_tokens, label=\"Neutral\", stop_words = stop_words )\n",
        "\n",
        "dataset = positive_dataset + negative_dataset + neutral_dataset\n",
        "random.shuffle(dataset)# mélange aleatoire du data set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Krm5wGtNBbyM",
        "colab_type": "text"
      },
      "source": [
        "Pour conclure nous obtenons un dataset labelisé et prêt etre utilisé avec les classifieurs nltk (Pour l'utilisation avec des classifieurs de bibliotèque plus classiques (sklearn,keras,...) quelques boucles suffisent). Bien qu'imparfait, ce format de dataset est utlisé très courramment et permet d'obtenir des resultats convenables que nous verrons dans le notebook Analyse_de_sentiment.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IFDeM-tKoGA",
        "colab_type": "text"
      },
      "source": [
        "##Documentation Utilisée\n",
        "\n",
        "* documentation de nltk : https://www.nltk.org/index.html\n",
        "* https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n"
      ]
    }
  ]
}