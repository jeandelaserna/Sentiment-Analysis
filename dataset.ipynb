{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skAHheaHo5Xn",
        "colab_type": "text"
      },
      "source": [
        "# FORMATION DU DATASET\n",
        "\n",
        "Ici nous nous allons travailler à construire un dataset grace à une suite de fonctions pour prétraiter des tweets. Le cadre d'utilisation de ces données sera l'analyse de sentiment mais on peut tout aussi bien adapter ce qui est développé ici à d'autres contextes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP4okbv2Awy3",
        "colab_type": "text"
      },
      "source": [
        "## Les Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWvQxTb0KJwV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "595c212f-d81c-44c8-c818-29c45c5cc794"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import re, string, random\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "# les stop word sont des mots telque ‘ourselves’, ‘hers’, ‘between’, ‘yourself’, ‘but’, ... \n",
        "# des mots assez courant pour apparaitre dans n'import quel texte"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWoebmZRBtLo",
        "colab_type": "text"
      },
      "source": [
        "Tout d'abord importons le dataset, nltk met à disposition plusieurs sets de tweets en langue anglaise avec les labels tweet neutres, positifs et négatifs, nous utiliserons ce qui est développé dans ce notebook pour l'apprentissage de nos classifieurs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzKp-7acLjy7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import du dataset et tokenization\n",
        "\n",
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "neutral_tweet_tokens = np.random.choice(twitter_samples.tokenized('tweets.20150430-223406.json'),size=len(negative_tweet_tokens),replace = False)\n",
        "\n",
        "#pour l'utilisation d'un dataset non equilibré:\n",
        "#neutral_tweet_tokens = twitter_samples.tokenized('tweets.20150430-223406.json')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWciRnCJA_1t",
        "colab_type": "text"
      },
      "source": [
        "## Le Pré-traitement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEjVp_1iC1Qc",
        "colab_type": "text"
      },
      "source": [
        "Ci-dessous quelques fonctions qui permettront de prétraiter les tweets.\n",
        "* Nous utilisons des tweets tokenisés, c'est dire que l'on passe d'un string à une liste de token, par exemple :  \n",
        "  * \"les vaches broutent dans le champ\" ent [\"les\", \"vache\", \"broutent\", \"dans\",  \"le\", \"champ\";\n",
        "* les tweets sont en premier lieu nettoyés des parties que nous jugions dispensables les adresses de sites, les noms, les chiffres, les nombres, les ponctuations, ... \n",
        "* Certains mots appelés stops word, sont aussi supprimés. Ce sont des mots qui apparaissent dans n'importe quel contexte, en français ce serait les mots le, la, les, à, ... en anglais ce sont leurs traductions ainsi que les auxiliaires utilisé pour les temps et d'autres plus spécifiques à l'anglais.(pour le detail de ces mots, regarder le contenu de la liste stop words)\n",
        "* et enfin les tokens restant son lemmetizés c’est-à-dire dans le cas du français la liste [\"cours\", \"courant\", \"courir\"] devient [\"cour\", \"cour\",\"cour\"] ou bien en anglais [\"Run\", \"Ran\", \"running\"] devient [\"Run\", \"Run\", \"Run\"] Ainsi nous réduisons le dictionnaire des mots utilisés sans pour autant perdre beaucoup de sens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rofD_vlKfS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_noise(tweet_tokens, stop_words = ()): #fait le nettoyage comme expliqué plus haut\n",
        "\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
        "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens\n",
        "\n",
        "\n",
        "def get_tweets_for_model(cleaned_tokens_list):# Nécessaire pour avoir le format requis pour les classifieurs de NLTK\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "def make_set( tweet_tokens, label=\"\", stop_words = () ) :# nettoie les tweets tokenisés et les associe à un label\n",
        "    cleaned_tokens_list = []\n",
        "    for tokens in tweet_tokens:\n",
        "        cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "    tokens_for_model = get_tweets_for_model(cleaned_tokens_list)\n",
        "    dataset = [(tweet_dict, label)for tweet_dict in tokens_for_model]\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls60ZGK2IFam",
        "colab_type": "text"
      },
      "source": [
        "Les défauts de cette manière de faire, sont :\n",
        "* ne prend pas en compte la position des mots dans la phrase.\n",
        "* les temps des phrases et les sens associés aux terminaisons des mots sont effacés.\n",
        "* les sens d'expression telque \"Tomber dans les pommes\" peut disparaitre.\n",
        "* ...\n",
        "\n",
        "Cette manière de binariser un texte est une parmi d'autres, on pourrait aussi travailler avec des n-uples de mots, par exemple avec des couples de mots:\n",
        "  * \"les vaches broutent l'herbe \" devient [\"les vaches\", \"les broutent\", \"les l'herbe\",\"vache broute\", \"vache l'herbe\", \"broute l'herbe\"]. \n",
        "  \n",
        "Ces manières-là peuvent réduire quelques défauts plus haut mais en apportent d'autres, une dimension plus grande des données, des combinaisons de mots qui n'apparaissent pratiquement jamais, ....\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tW68Y2MKh85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  nettoyage et formation du dataset\n",
        "\n",
        "positive_dataset = make_set( positive_tweet_tokens, label=\"Positive\", stop_words = stop_words )\n",
        "negative_dataset = make_set( negative_tweet_tokens, label=\"Negative\", stop_words = stop_words )\n",
        "neutral_dataset = make_set( neutral_tweet_tokens, label=\"Neutral\", stop_words = stop_words )\n",
        "\n",
        "dataset = positive_dataset + negative_dataset + neutral_dataset\n",
        "random.shuffle(dataset)# mélange aleatoire du data set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Krm5wGtNBbyM",
        "colab_type": "text"
      },
      "source": [
        "Pour conclure nous obtenons un dataset labellisé et prêt être utilisé avec les classifieurs nltk (Pour l'utilisation avec des classifieurs de bibliothèque plus classiques (sklearn, kermas.....) quelques boucles suffisent). Bien qu'imparfait, ce format de dataset est utilisé très couramment et permet d'obtenir des résultats convenables que nous verrons dans le notebook Analyse_de_Sentiment.ipynb\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IFDeM-tKoGA",
        "colab_type": "text"
      },
      "source": [
        "##Documentation Utilisée\n",
        "\n",
        "* documentation de nltk : https://www.nltk.org/index.html\n",
        "* https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n"
      ]
    }
  ]
}